# # from transformers import BertTokenizer, BertForQuestionAnswering
# # import torch

# # tokenizer = BertTokenizer.from_pretrained('./model/chinese_bert_wwm')
# # # model = BertForQuestionAnswering.from_pretrained('./model/chinese_bert_wwm')
# # # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
# # # print(device)
# # # model.to(device)

# # # question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
# # # inputs = tokenizer(question, text, return_tensors='pt')
# # # start_positions = torch.tensor([1])
# # # end_positions = torch.tensor([3])

# # # outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
# # # loss = outputs.loss
# # # start_scores = outputs.start_logits
# # # end_scores = outputs.end_logits
# # feature = [[101, 1825, 4794, 1920, 7481, 4916, 1103, 1170, 510, 4788, 2938, 4638, 712, 6206, 1333, 1728, 8043, 102, 8020, 122, 8021, 4567, 2154, 2989, 6835, 8038, 122, 108, 1875, 1825, 4794, 698, 7028, 1103, 1170, 510, 4788, 2938, 8024, 7481, 4916, 711, 124, 119, 121, 11916, 8039, 121, 108, 1378, 510, 123, 108, 1378, 1825, 4794, 6768, 2544, 1103, 6008, 511, 8020, 123, 8021, 4567, 2154, 1146, 3358, 8038, 3441, 3448, 2456, 2768, 2399, 807, 719, 6823, 8024, 6421, 3441, 1905, 754, 2255, 6484, 856, 3834, 1905, 8024, 7433, 2108, 1920, 7030, 7433, 3717, 3726, 7415, 1931, 3942, 4767, 1779, 8024, 4684, 2970, 1103, 1170, 1825, 4794, 8024, 4294, 1166, 3221, 855, 754, 3777, 6887, 704, 1925, 4638, 3441, 1875, 1825, 4794, 1358, 1168, 4638, 1103, 1140, 3209, 3227, 8024, 7270, 3309, 6817, 5852, 2193, 5636, 1825, 4794, 1920, 7481, 4916, 1103, 1170, 510, 4788, 2938, 511, 8020, 124, 8021, 1905, 3780, 2456, 6379, 8038, 2190, 1825, 4794, 1103, 1170, 4567, 2154, 6822, 6121, 1905, 3780, 8024, 2400, 6858, 6814, 2193, 3837, 510, 7344, 2844, 5023, 3175, 2466, 7344, 3632, 1103, 1170, 4567, 2154, 1086, 3613, 1139, 4385, 1469, 1217, 1196, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 78, 106, 2]
# # print(feature[0][feature[3]:feature[4]+1])
# # tokens_list = tokenizer.convert_ids_to_tokens(feature[0][feature[3]:feature[4]+1])
# # print(''.join(token for token in tokens_list))

import psutil

mem = psutil.virtual_memory()
# 系统总计内存
zj = float(mem.total) / 1024 / 1024 / 1024
# 系统已经使用内存
ysy = float(mem.used) / 1024 / 1024 / 1024

# 系统空闲内存
kx = float(mem.free) / 1024 / 1024 / 1024

print('系统总计内存:%d.3GB' % zj)
print('系统已经使用内存:%d.3GB' % ysy)
print('系统空闲内存:%d.3GB' % kx)

# epochs_2807 = []
# epochs_3637 = []
# for i in range(1, 11):
#     epochs_2807.append(3807*10*i//3009)
#     epochs_3637.append(3807*10*i//3637)

# print(epochs_2807)
# print(epochs_3637)